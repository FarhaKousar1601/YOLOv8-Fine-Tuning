{"cells":[{"cell_type":"code","execution_count":1,"id":"4961e54c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4961e54c","executionInfo":{"status":"ok","timestamp":1721990235467,"user_tz":-300,"elapsed":757,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}},"outputId":"0d4fb127-1766-4e6d-c0b0-747f58d3427c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}],"source":["!python --version"]},{"cell_type":"code","execution_count":2,"id":"5ea3ead0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"5ea3ead0","executionInfo":{"status":"ok","timestamp":1721990307548,"user_tz":-300,"elapsed":68850,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}},"outputId":"65cd3a1b-2785-4c42-d099-6262404eb185"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.2.66-py3-none-any.whl.metadata (41 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.25.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Downloading ultralytics-8.2.66-py3-none-any.whl (825 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.5/825.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n","Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.66 ultralytics-thop-2.0.0\n"]},{"output_type":"execute_result","data":{"text/plain":["'8.2.66'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["!pip install ultralytics\n","import ultralytics\n","ultralytics.__version__"]},{"cell_type":"code","execution_count":3,"id":"6801a5a9","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"6801a5a9","executionInfo":{"status":"ok","timestamp":1721990307550,"user_tz":-300,"elapsed":24,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}},"outputId":"3946dbc9-274e-4745-bdbe-60cb9cc2d2bd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.3.1+cu121'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["import torch\n","torch.__version__"]},{"cell_type":"code","execution_count":4,"id":"9627ba4a","metadata":{"id":"9627ba4a","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1721990307551,"user_tz":-300,"elapsed":20,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}},"outputId":"ffb1c196-1f6f-4ed2-9675-3a244642366e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla T4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["torch.cuda.get_device_name(0)"]},{"cell_type":"markdown","id":"30beac26","metadata":{"id":"30beac26"},"source":["# Detect, track and count Persons"]},{"cell_type":"code","execution_count":6,"id":"c23349aa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c23349aa","executionInfo":{"status":"ok","timestamp":1721992052914,"user_tz":-300,"elapsed":436,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}},"outputId":"9ed240be-b965-4e2d-c9c2-10136d6caa95"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-main\n"]}],"source":["%cd /content/drive/MyDrive/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-main"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wreAzMe1R7PJ","executionInfo":{"status":"ok","timestamp":1721990386838,"user_tz":-300,"elapsed":73766,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}},"outputId":"cb1a389d-badb-4a49-b45c-d0afd4c57b6c"},"id":"wreAzMe1R7PJ","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from ultralytics import YOLO\n","import os\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","# Load a model\n","model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n","\n","# Define the class names\n","class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n","\n","# Folder containing images\n","image_folder = \"/content/drive/MyDrive/YOLO-HiVis-Data/train/images\"\n","image_files = os.listdir(image_folder)[:100]  # Process only the first 100 images\n","\n","# Function to plot the results\n","def plot_image_with_boxes(image, boxes, class_names):\n","    for box in boxes:\n","        xyxy = box.xyxy[0].cpu().numpy()\n","        cls = int(box.cls[0])\n","        conf = box.conf[0].cpu().numpy()\n","        class_name = class_names[cls]\n","        if class_name == 'person':  # Filter only the \"person\" class\n","            x1, y1, x2, y2 = map(int, xyxy)\n","            label = f\"{class_name} {conf:.2f}\"\n","            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n","            cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n","    return image\n","\n","# Process each image\n","for img_file in image_files:\n","    img_path = os.path.join(image_folder, img_file)\n","    results = model(img_path)\n","    for result in results:\n","        image = cv2.imread(img_path)\n","        image = plot_image_with_boxes(image, result.boxes, class_names)\n","        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","        plt.axis('off')\n","        plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1-ZvVmFqZKv-t7-kX2PnzfecwBXUDFW2N"},"id":"h5hj7cZHuYrd","executionInfo":{"status":"ok","timestamp":1721992315949,"user_tz":-300,"elapsed":234973,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}},"outputId":"19cbd516-bf6f-4f5e-ff06-bbb389aac5ad"},"id":"h5hj7cZHuYrd","execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# \"/content/drive/MyDrive/YOLO-HiVis-Data/train/images\""],"metadata":{"id":"hKTneqznYJuI"},"id":"hKTneqznYJuI","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":8,"id":"7ac57944","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ac57944","executionInfo":{"status":"ok","timestamp":1721992455189,"user_tz":-300,"elapsed":2522,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}},"outputId":"98151686-1afc-48af-fcf0-b2fcebca4cad"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/drive/MyDrive/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-main/images/person.jpg: 384x640 1 person, 115.9ms\n","Speed: 9.0ms preprocess, 115.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n","[0.0]\n","Class: person\n"]}],"source":["from ultralytics import YOLO\n","\n","import time\n","import torch\n","import cv2\n","import torch.backends.cudnn as cudnn\n","from PIL import Image\n","import colorsys\n","import numpy as np\n","\n","# Load a model\n","model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n","\n","results = model(\"images/person.jpg\", save=True)\n","\n","\n","\n","class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n","\n","for result in results:\n","    boxes = result.boxes  # Boxes object for bbox outputs\n","    probs = result.probs  # Class probabilities for classification outputs\n","    cls = boxes.cls.tolist()  # Convert tensor to list\n","    xyxy = boxes.xyxy\n","    xywh = boxes.xywh  # box with xywh format, (N, 4)\n","    conf = boxes.conf\n","    print(cls)\n","    for class_index in cls:\n","        class_name = class_names[int(class_index)]\n","        print(\"Class:\", class_name)"]},{"cell_type":"markdown","id":"461c7b6e","metadata":{"id":"461c7b6e"},"source":["# DeepSORT"]},{"cell_type":"code","execution_count":9,"id":"945f584b","metadata":{"id":"945f584b","executionInfo":{"status":"ok","timestamp":1721992521039,"user_tz":-300,"elapsed":10518,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}}},"outputs":[],"source":["from deep_sort.utils.parser import get_config\n","from deep_sort.deep_sort import DeepSort\n","from deep_sort.sort.tracker import Tracker\n","\n","deep_sort_weights = 'deep_sort/deep/checkpoint/ckpt.t7'\n","tracker = DeepSort(model_path=deep_sort_weights, max_age=70)"]},{"cell_type":"code","execution_count":10,"id":"2d74f1e2","metadata":{"id":"2d74f1e2","executionInfo":{"status":"ok","timestamp":1721992528190,"user_tz":-300,"elapsed":1147,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}}},"outputs":[],"source":["# Define the video path\n","video_path = 'videos/Student walking a University.mp4'\n","\n","cap = cv2.VideoCapture(video_path)\n","\n","# Get the video properties\n","frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","# Define the codec and create VideoWriter object\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","output_path = 'output.mp4'\n","out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":11,"id":"09056afd","metadata":{"id":"09056afd","executionInfo":{"status":"ok","timestamp":1721992532570,"user_tz":-300,"elapsed":424,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}}},"outputs":[],"source":["frames = []\n","\n","unique_track_ids = set()"]},{"cell_type":"code","source":["from ultralytics import YOLO\n","import cv2\n","import numpy as np\n","\n","# Load a model\n","model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n","\n","# Define the class names\n","class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n","\n","# Function to plot the results\n","def plot_frame_with_boxes(frame, boxes, class_names):\n","    person_count = 0\n","    for box in boxes:\n","        xyxy = box.xyxy[0].cpu().numpy()\n","        cls = int(box.cls[0])\n","        conf = box.conf[0].cpu().numpy()\n","        class_name = class_names[cls]\n","        if class_name == 'person':  # Filter only the \"person\" class\n","            person_count += 1\n","            x1, y1, x2, y2 = map(int, xyxy)\n","            label = f\"{class_name} {conf:.2f}\"\n","            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n","            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n","    return frame, person_count\n","\n","# Video file path\n","video_path = \"videos/Student walking a University.mp4\"\n","\n","# Open the video file\n","cap = cv2.VideoCapture(video_path)\n","\n","# Check if video opened successfully\n","if not cap.isOpened():\n","    print(\"Error: Could not open video.\")\n","    exit()\n","\n","# Get video frame dimensions\n","frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n","\n","# Define the codec and create VideoWriter object\n","output_path = \"output_video.mp4\"\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","out = cv2.VideoWriter(output_path, fourcc, frame_rate, (frame_width, frame_height))\n","\n","# Process each frame\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    results = model(frame)\n","    for result in results:\n","        frame, person_count = plot_frame_with_boxes(frame, result.boxes, class_names)\n","        cv2.putText(frame, f\"Persons: {person_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n","\n","    # Write the frame with detection boxes\n","    out.write(frame)\n","\n","    # Display the frame\n","    # cv2.imshow('Video', frame)\n","    # if cv2.waitKey(1) & 0xFF == ord('q'):\n","    #     break\n","\n","# Release video capture and writer objects\n","cap.release()\n","out.release()\n","cv2.destroyAllWindows()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YaMcIHWO1qgk","executionInfo":{"status":"ok","timestamp":1721994057681,"user_tz":-300,"elapsed":5728,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}},"outputId":"1b7a10f2-e94e-4cfc-c8a0-0247332d2cf4"},"id":"YaMcIHWO1qgk","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 6.9ms\n","Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 8.2ms\n","Speed: 1.8ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 7.7ms\n","Speed: 2.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.6ms\n","Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 8.8ms\n","Speed: 2.1ms preprocess, 8.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 7.5ms\n","Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 7.4ms\n","Speed: 1.9ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 2 backpacks, 2 handbags, 7.8ms\n","Speed: 1.9ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 2 backpacks, 2 handbags, 6.9ms\n","Speed: 1.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 2 backpacks, 2 handbags, 6.7ms\n","Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 9.4ms\n","Speed: 1.8ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 10.7ms\n","Speed: 2.0ms preprocess, 10.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 9.3ms\n","Speed: 2.3ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 10.1ms\n","Speed: 2.0ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 5 handbags, 9.6ms\n","Speed: 2.2ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 6 handbags, 9.4ms\n","Speed: 2.2ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 5 handbags, 13.4ms\n","Speed: 3.0ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.4ms\n","Speed: 2.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.1ms\n","Speed: 2.0ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 2 backpacks, 4 handbags, 12.8ms\n","Speed: 2.4ms preprocess, 12.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.2ms\n","Speed: 2.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 11.2ms\n","Speed: 2.6ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 5 handbags, 7.7ms\n","Speed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 8.9ms\n","Speed: 2.1ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.9ms\n","Speed: 2.1ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 2 backpacks, 4 handbags, 7.8ms\n","Speed: 1.9ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 2 backpacks, 4 handbags, 7.4ms\n","Speed: 2.3ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 2 backpacks, 4 handbags, 7.2ms\n","Speed: 2.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 2 backpacks, 4 handbags, 7.9ms\n","Speed: 2.0ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 2 backpacks, 5 handbags, 7.4ms\n","Speed: 2.4ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 5 handbags, 7.0ms\n","Speed: 2.0ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 6.9ms\n","Speed: 1.7ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.5ms\n","Speed: 2.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 8.3ms\n","Speed: 2.1ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.5ms\n","Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 7.0ms\n","Speed: 1.6ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 8.6ms\n","Speed: 2.0ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 10.2ms\n","Speed: 1.8ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 7.0ms\n","Speed: 2.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 15.0ms\n","Speed: 2.3ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 13.8ms\n","Speed: 3.5ms preprocess, 13.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 1 handbag, 13.6ms\n","Speed: 2.1ms preprocess, 13.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 10.4ms\n","Speed: 1.9ms preprocess, 10.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 10.0ms\n","Speed: 3.1ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 10.5ms\n","Speed: 1.8ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 9.7ms\n","Speed: 2.8ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 2 backpacks, 3 handbags, 9.4ms\n","Speed: 1.9ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 9.5ms\n","Speed: 5.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 13.7ms\n","Speed: 3.2ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 11.3ms\n","Speed: 1.9ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 6.9ms\n","Speed: 2.0ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 8.3ms\n","Speed: 1.9ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 7.7ms\n","Speed: 2.2ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 7.3ms\n","Speed: 1.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 9.9ms\n","Speed: 2.2ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 7.7ms\n","Speed: 2.0ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 2 backpacks, 3 handbags, 8.6ms\n","Speed: 2.7ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 7.4ms\n","Speed: 2.5ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 7.2ms\n","Speed: 2.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 8.1ms\n","Speed: 2.6ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 7.5ms\n","Speed: 2.3ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 7.1ms\n","Speed: 1.9ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 7.2ms\n","Speed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 8.3ms\n","Speed: 2.2ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 10.5ms\n","Speed: 2.5ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 10.1ms\n","Speed: 3.0ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 12.0ms\n","Speed: 1.9ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 9.2ms\n","Speed: 1.9ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 9.7ms\n","Speed: 1.7ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 9.9ms\n","Speed: 1.8ms preprocess, 9.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 9.8ms\n","Speed: 2.9ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 9.2ms\n","Speed: 2.3ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 11.1ms\n","Speed: 1.8ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 9.3ms\n","Speed: 3.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 9.1ms\n","Speed: 2.2ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.9ms\n","Speed: 1.9ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.4ms\n","Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 8.2ms\n","Speed: 1.8ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 7.5ms\n","Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.5ms\n","Speed: 1.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 5 handbags, 8.2ms\n","Speed: 1.7ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.6ms\n","Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 5 handbags, 6.9ms\n","Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 4 backpacks, 6 handbags, 7.7ms\n","Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 5 handbags, 7.8ms\n","Speed: 1.6ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.6ms\n","Speed: 3.2ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 3 backpacks, 5 handbags, 6.7ms\n","Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 5 handbags, 7.7ms\n","Speed: 1.6ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 6.8ms\n","Speed: 1.8ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.7ms\n","Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 8.5ms\n","Speed: 3.0ms preprocess, 8.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 8.3ms\n","Speed: 1.8ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 8.2ms\n","Speed: 1.6ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 9.2ms\n","Speed: 1.8ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 11.3ms\n","Speed: 3.7ms preprocess, 11.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 10.9ms\n","Speed: 3.7ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 9.8ms\n","Speed: 1.8ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 11.3ms\n","Speed: 1.9ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 10.2ms\n","Speed: 2.2ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 10.2ms\n","Speed: 4.5ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 9.4ms\n","Speed: 2.3ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 9.7ms\n","Speed: 3.0ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 9.5ms\n","Speed: 2.6ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 10.9ms\n","Speed: 4.2ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 11.0ms\n","Speed: 2.0ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 11.4ms\n","Speed: 2.1ms preprocess, 11.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 8.9ms\n","Speed: 2.1ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 7.6ms\n","Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 6.8ms\n","Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 10.0ms\n","Speed: 2.0ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 10.6ms\n","Speed: 1.9ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 11.0ms\n","Speed: 3.7ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 4 handbags, 15.2ms\n","Speed: 1.7ms preprocess, 15.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 12.1ms\n","Speed: 1.8ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 15.2ms\n","Speed: 1.8ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 12.3ms\n","Speed: 1.8ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 9.7ms\n","Speed: 1.9ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 12.0ms\n","Speed: 1.7ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 11.3ms\n","Speed: 2.7ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 11.7ms\n","Speed: 1.7ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 9.6ms\n","Speed: 2.3ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 11.5ms\n","Speed: 1.7ms preprocess, 11.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 12.1ms\n","Speed: 1.8ms preprocess, 12.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 14.8ms\n","Speed: 2.0ms preprocess, 14.8ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 13.7ms\n","Speed: 2.1ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 3 handbags, 9.8ms\n","Speed: 2.6ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 9.3ms\n","Speed: 1.8ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 backpacks, 2 handbags, 9.5ms\n","Speed: 1.6ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]}]},{"cell_type":"code","execution_count":15,"id":"533ff5cc","metadata":{"id":"533ff5cc","executionInfo":{"status":"ok","timestamp":1721994504878,"user_tz":-300,"elapsed":930,"user":{"displayName":"Nabeel Arain","userId":"14991964424198148578"}}},"outputs":[],"source":["# i = 0\n","# counter, fps, elapsed = 0, 0, 0\n","# start_time = time.perf_counter()\n","\n","# while cap.isOpened():\n","#     ret, frame = cap.read()\n","\n","#     if ret:\n","\n","#         og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","#         frame = og_frame.copy()\n","\n","#         model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n","\n","#         results = model(frame, device=device, classes=0, conf=0.8)\n","\n","#         class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n","\n","#         for result in results:\n","#             boxes = result.boxes  # Boxes object for bbox outputs\n","#             probs = result.probs  # Class probabilities for classification outputs\n","#             cls = boxes.cls.tolist()  # Convert tensor to list\n","#             xyxy = boxes.xyxy\n","#             conf = boxes.conf\n","#             xywh = boxes.xywh  # box with xywh format, (N, 4)\n","#             for class_index in cls:\n","#                 class_name = class_names[int(class_index)]\n","#                 #print(\"Class:\", class_name)\n","\n","#         pred_cls = np.array(cls)\n","#         conf = conf.detach().cpu().numpy()\n","#         xyxy = xyxy.detach().cpu().numpy()\n","#         bboxes_xywh = xywh\n","#         bboxes_xywh = xywh.cpu().numpy()\n","#         bboxes_xywh = np.array(bboxes_xywh, dtype=float)\n","\n","#         tracks = tracker.update(bboxes_xywh, conf, og_frame)\n","\n","#         for track in tracker.tracker.tracks:\n","#             track_id = track.track_id\n","#             hits = track.hits\n","#             x1, y1, x2, y2 = track.to_tlbr()  # Get bounding box coordinates in (x1, y1, x2, y2) format\n","#             w = x2 - x1  # Calculate width\n","#             h = y2 - y1  # Calculate height\n","\n","#             # Set color values for red, blue, and green\n","#             red_color = (0, 0, 255)  # (B, G, R)\n","#             blue_color = (255, 0, 0)  # (B, G, R)\n","#             green_color = (0, 255, 0)  # (B, G, R)\n","\n","#             # Determine color based on track_id\n","#             color_id = track_id % 3\n","#             if color_id == 0:\n","#                 color = red_color\n","#             elif color_id == 1:\n","#                 color = blue_color\n","#             else:\n","#                 color = green_color\n","\n","#             cv2.rectangle(og_frame, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n","\n","#             text_color = (0, 0, 0)  # Black color for text\n","#             cv2.putText(og_frame, f\"{class_name}-{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1, cv2.LINE_AA)\n","\n","#             # Add the track_id to the set of unique track IDs\n","#             unique_track_ids.add(track_id)\n","\n","#         # Update the person count based on the number of unique track IDs\n","#         person_count = len(unique_track_ids)\n","\n","#         # Update FPS and place on frame\n","#         current_time = time.perf_counter()\n","#         elapsed = (current_time - start_time)\n","#         counter += 1\n","#         if elapsed > 1:\n","#             fps = counter / elapsed\n","#             counter = 0\n","#             start_time = current_time\n","\n","#         # Draw person count on frame\n","#         cv2.putText(og_frame, f\"Person Count: {person_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n","\n","#         # Append the frame to the list\n","#         frames.append(og_frame)\n","\n","#         # Write the frame to the output video file\n","#         out.write(cv2.cvtColor(og_frame, cv2.COLOR_RGB2BGR))\n","\n","#         # Show the frame\n","#         #cv2.imshow(\"Video\", og_frame)\n","# #         if cv2.waitKey(1) & 0xFF == ord('q'):\n","# #             break\n","\n","# cap.release()\n","# out.release()\n","# cv2.destroyAllWindows()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}